## Go Fish with RL Agent
This is a program for an online Go Fish game, where a human player plays against a Reinforcement Learning agent. The game state logic and RL agent models were used together with an interactive Streamlit interface to shape a realisitic setting.  

**Link to game:** https://gofishonlinekbeach25.streamlit.app  

## Development
There were three major elements to building this: the Go Fish environment, training and testing the RL agents, and the Streamlit interface.  

The Go Fish environment, GoFishEnv.py, was built using gymnasium to provide the structure and make it compatible with Stable Baselines3, the Reinforcement Learning pipeline used for training the agents. All game logic is handled within this environment, and reward calculation incentivizes intelligent gameplay and penalizes illegal actions and poor strategies. The environment strucutre made implementing it into the Streamlit interface simple.   

The second step was training and testing the RL agents. I had originally trained the agents against a random opponent, which led to very high win rates when I simulated 10,000 games. Despite the win rates, I found that they didn't play too well against me, since I wasn't just picking random cards. So, I made the training opponent more strategic. This led to an overall decrease in win rates, but much more realistic performance when I played agsinst them. I used three scripts for this step: test_agent.py, play_agent.py, and evaluate.py. My test_agent.py script used Proximal Policy Optimization (PPO) from Stable Baselines3 to train agents in my environment. Go Fish is a fairly simple game, so I felt that PPO was a good choice since it handles simple to low complexity tasks well. Agents used for higher difficulties trained with more timesteps. I used play_agent to play against each agent in my terminal, and I made their hand visible to me so I could monitor if they were making intelligent and legal moves. I used evaluate.py to simulate 10,000 games to establish success rates for each model. Each agent played against my heuristic-based opponent, and easy, medium, and hard difficulties achieved win rates of 59.00%, 66.29%, and 74.15% respectively.  

The last step was building and deploying the Streamlit app. My script for this app, app.py, gets a deck of cards from [deckofcardsapi.com ](https://deckofcardsapi.com) and provides a real-time visual of the current game state. It allows you to choose from easy, medium, and hard, and sets the appropriate model depending on the selected difficulty. Once a player has completed 7 sets, the game ends, since it's impossible for the opponent to get more than 6 at that point. You have the option to play again, which takes you back to the landing page and you can choose to play the same difficulty or another one. 

## Challenges
Building the environment required a lot of attention to detail. Small errors in game logic were easy to miss when writing the code, but were impossible to miss when I would play test games. I'd written down as much of the game logic as I could ahead of time which helped, but I still ran into game-breaking exceptions occasionally. Also, when I initially set up the environment, I defined the card suit range from 0-13, knowing I'd have to convert it from 2-ACE later on. This seemed like a good idea to me initially since it was all numbers, but it ended up causing more problems than I anticipated. If I were to do this project again, I would have started by setting up the suit range as 2-ACE and handled the issue of not all the suits being numbers immediately, or I would have set up the initial range as 2-15 so nothing except the face cards required change.  

The majority of challenges pertaining to training, testing, and evaluating the agents stemmed from the environment. I hadn't built in illegal move handling at first, so I'd see the agent asking for cards it didn't have. The biggest challenge was handling early convergence. The model would converge early several times while testing after it found some exploitable loophole in the environment or when my reward system wasn't properly balaneced. This issue led to a lot of trial and error with the observation space, the number of timesteps, and the reward system. Also, as I mentioned above, the quality of opponent made evaluating the model's win rate difficult. The original hard difficulty model won about 81% of the time against a completely random opponent, but was not very effective against a human player. I changed the opponent to make more strategic moves, and the model's win rate went down to about 74%, but the performance against human players was much better.  

Streamlit itself is not a big challenge, but pairing it with the environment and game state was difficult at times. The biggest challenge in this section was the hand visible to the player not being synced with the hand from the game state. This happened for all sorts of reasons, and I had to alter function calls and hand update timings to fix this issue. Hand desyncing paired with the game envronment/card visual rank discrepancy was probably the biggest challenge in this project. Another challenge, although it was a simple one, was handling visual cluttering on the game page. I had initially used time.sleep(x) to control the pace of the game, but this caused Streamlit rerun() calls to struggle when trying to update the game state visual. So, I removed the instances of time.sleep(x) and replaced them with buttons to advance the game. 
